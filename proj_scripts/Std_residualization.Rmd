---
title: "Residualization reinterpretation"
author:
  - name: "Nicholas Judd"
    url: https://staff.ki.se/people/nicholas-judd
    affiliation: Karolinska Institute
    affiliation_url: https://ki.se/en
    orcid_id: 0000-0002-0196-9871
output: distill::distill_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(MASS); library(data.table);library(kableExtra); library(equatiomatic); 
library(sjPlot); library(lm.beta)
```

## How standardized effects vary with residualization

The [Frisch-Waugh-Lovell theorem](https://en.wikipedia.org/wiki/Frisch%E2%80%93Waugh%E2%80%93Lovell_theorem) theorem states that residualizing all variables in the linear model for a variable (e.g., X2) is equal to adding it as a covariate. Therefore B1 in the two equations are identical:

$$
\operatorname{Y} = \alpha + \beta_{1}(\operatorname{X1}) + \beta_{2}(\operatorname{X2}) + \epsilon
$$
$$
\operatorname{Y_{residualized\_X2}} = \alpha + \beta_{1}(\operatorname{X1_{residualized\_X2}}) + \epsilon
$$


This is a quick simulation showing the Frisch-Waugh-Lovell with two correlated predictors. It also shows how residualizing only the dependent variable leads to a different result and how rescaling can artificially inflate your standardized effect.


### Data simulation

```{r sim data and data generation, echo=FALSE}
sig <- matrix(c(1.0, 0.8, 0.5, 0.3, #DV
                0.8, 1.0, 0.5, 0.7, # PGS
                0.5, 0.5, 1.0, 0.3,
                0.3, 0.7, 0.3, 1), # SES
              nrow = 4)

x <- data.frame(mvrnorm(n = 1000, mu = rep(0, 4), Sigma = sig, empirical = TRUE))

colnames(x) <- c('Y', "X1", "X2", "X3")
cr <- cor(x) %>% round(2)

# making variables with X2 residualized
x$Y_X3res <- as.data.frame(lm(Y ~ X3, data = x)$residuals)[,1]
x$X1_X3res <- as.data.frame(lm(X1 ~ X3, data = x)$residuals)[,1]
x$X2_X3res <- as.data.frame(lm(X2 ~ X3, data = x)$residuals)[,1]


# now I just standardize the residuals; just using the tab_model function to show my point
# c <- c("Y_X3res", "X1_X3res", "X2_X3res") # cols to scale (mean = 0, sd = 1)
# setDT(x)
# x[, (paste(c, "_Scaled", sep="")) := lapply(.SD, function(x) as.numeric(scale(x))), .SDcols=c]

```


First we simulate data of with 1000 subjects:

 - 4 correlated variables in standard units (mean = 0, sd = 1)
 - Y reflects the dependent variable while X1, X2 & X3 are predictors
 - We have X3 as a predictor in three linear models with Y, X1 & X2 and extract the residuals
 - These variables are coded as Y_X3res, X1_X3res & X2_X3res



```{r corrplot, out.width= "65%", out.extra='style="float:right; padding:10px"', echo = F}
corrplot::corrplot.mixed(cr, number.cex=2, tl.cex = 2)
```


```{r descriptives, echo=FALSE, include=F}
as.data.frame(psych::describe(x))%>% round(2) %>% dplyr::select(n, mean, sd) %>% kbl(caption = "An overview of simluated variables") %>% kable_styling()
```


### Model comparison

Here we show the equivalence between model *** and ***. X1's effect on Y, is of equal magnitude along with the SE. 

```{r standard models, echo=FALSE}

m1 <- lm(Y ~ X1 + X3, data = x)
m2 <- lm(Y_X3res ~ X1_X3res, data = x)


tab_model(m1, m2, show.se = T, show.std = T, show.ci = F, show.p = F)
```

It is quite common to standardize to get interpretable effect sizes, yet if we standardize after already residualizing we are inflating our effect sizes because it is now the variance in Y neglecting X2's contribution (the "pie" is smaller). This issue can sneak up on you, for example if you fit a structural equation model where you residualized the variable for age and now you standardize it. 


$$
 \beta_{1}(\operatorname{SD_{X}})/SD_{Y}
$$


Why is it not an issue with a single var, is it just cause it's not correlated enough???



```{r accidently standardizing, echo = F}


m3 <- lm(Y ~ X1 + X2 + X3, data = x)
m4 <- lm(Y_X3res ~ X1_X3res + X2_X3res, data = x)


tab_model(m3, m4, show.se = T, show.std = T, show.ci = F, show.p = F)
```



```{r problem as a function of corelation?, echo=F}




```



