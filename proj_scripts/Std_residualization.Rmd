---
title: "Residualization reinterpretation"
author:
  - name: "Nicholas Judd"
    url: https://staff.ki.se/people/nicholas-judd
    affiliation: Karolinska Institute
    affiliation_url: https://ki.se/en
    orcid_id: 0000-0002-0196-9871
output: distill::distill_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(MASS); library(data.table);library(kableExtra); library(equatiomatic); 
library(sjPlot); library(lm.beta)
```

## How standardized effects vary with residualization

The [Frisch-Waugh-Lovell theorem](https://en.wikipedia.org/wiki/Frisch%E2%80%93Waugh%E2%80%93Lovell_theorem) theorem states that residualizing all variables in the linear model for a variable (e.g., X2) is equal to adding it as a covariate. 

Therefore B1 in the two equations are identical:

$$
\operatorname{Y} = \alpha + \beta_{1}(\operatorname{X1}) + \beta_{2}(\operatorname{X2}) + \epsilon
$$
$$
\operatorname{Y_{residualized\_X2}} = \alpha + \beta_{1}(\operatorname{X1_{residualized\_X2}}) + \epsilon
$$


This is a quick simulation showing the Frisch-Waugh-Lovell with two correlated predictors. It also shows how residualizing only the dependent variable leads to a different result and how rescaling can artificially inflate your standardized effect.


### Data simulation

```{r sim data and data generation, echo=FALSE}
sig <- matrix(c(1.0, 0.5, 0.6,
                0.5, 1.0, 0.3,
                0.6, 0.3, 1.0),
              nrow = 3)

x <- data.frame(mvrnorm(n = 1000, mu = rep(0, 3), Sigma = sig, empirical = TRUE))

colnames(x) <- c('Y', "X1", "X2")#, "X3")
cr <- cor(x) %>% round(2)

# making variables with X2 residualized
x$Y_X2res <- as.data.frame(lm(Y ~ X2, data = x)$residuals)[,1]
x$X1_X2res <- as.data.frame(lm(X1 ~ X2, data = x)$residuals)[,1]
#x$X2_X3res <- as.data.frame(lm(X2 ~ X3, data = x)$residuals)[,1]


# now I just standardize the residuals; just using the tab_model function to show my point
# c <- c("Y_X3res", "X1_X3res", "X2_X3res") # cols to scale (mean = 0, sd = 1)
# setDT(x)
# x[, (paste(c, "_Scaled", sep="")) := lapply(.SD, function(x) as.numeric(scale(x))), .SDcols=c]

```


First we simulate data of with 1000 subjects:

::: {.floatting}

```{r corrplot, out.width= "50%", out.extra='style="float:right; padding:0px"', echo = F}
corrplot::corrplot.mixed(cr, number.cex=2, tl.cex = 2)
```
 - 3 correlated variables in standard units (mean = 0, sd = 1)
 - Y reflects the dependent variable while X1 & X2 are predictors
 - We have X2 as a predictor in three linear models with Y & X1 and extract the residuals
 - These variables are coded as Y_X2res & X1_X2res
:::

```{r descriptives, echo=FALSE, include=F, eval = F}
as.data.frame(psych::describe(x))%>% round(2) %>% dplyr::select(n, mean, sd) %>% kbl(caption = "An overview of simluated variables") %>% kable_styling()
```


```{r fit models, echo = F}
m1 <- lm(Y ~ X1 + X2, data = x)
m2 <- lm(Y_X2res ~ X1, data = x)
m3 <- lm(Y_X2res ~ X1_X2res, data = x)
```

### 1. Frisch-Waugh-Lovell

In the table below we can see that the effect size and SE (of X1) are the same when we residualize both the dependent and independent variable.

```{r table FWL, echo=FALSE}
tab_model(m1, m3, show.se = T, 
          show.ci = F, show.p = F, show.r2 = F)
```

### 2. Scaling inflates effect sizes

If we standardize our residualized model it will inflate the effect sizes. This is because we are rescaling it (see equation below). While this may seem obvious at first it can sneak up on you, for example if you fit a structural equation model where you residualized the variable for age and now you standardize it. 

$$
 \beta_{1}(\operatorname{SD_{X}})/SD_{Y}
$$

```{r table std, echo=FALSE}
tab_model(m1, m3, show.se = T, show.std = T, 
          show.ci = F, show.p = F, show.r2 = F)
```

### 3. Residualizing only the DV changes our interpretation

When we only residualize the dependent variable it changes the meaning of the other term when the two are related.


```{r table dv, echo=FALSE}
tab_model(m1, m2, show.se = T, show.std = T, 
          show.ci = F, show.p = F, show.r2 = F)
```


```{r accidently standardizing, echo = F, include=F, eval = F}


m3 <- lm(Y ~ X1 + X2 + X3, data = x)
m4 <- lm(Y_X3res ~ X1_X3res + X2_X3res, data = x)


tab_model(m3, m4, show.se = T, show.std = T, show.ci = F, show.p = F)
```

